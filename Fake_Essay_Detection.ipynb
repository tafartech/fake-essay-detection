{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 61542,
          "databundleVersionId": 6888007,
          "sourceType": "competition"
        },
        {
          "sourceId": 6865136,
          "sourceType": "datasetVersion",
          "datasetId": 3945154
        },
        {
          "sourceId": 6890527,
          "sourceType": "datasetVersion",
          "datasetId": 3942644
        },
        {
          "sourceId": 6901341,
          "sourceType": "datasetVersion",
          "datasetId": 3960967
        },
        {
          "sourceId": 6977472,
          "sourceType": "datasetVersion",
          "datasetId": 4005256
        }
      ],
      "dockerImageVersionId": 30627,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Fake Essay Detection",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tafartech/fake-essay-detection/blob/main/Fake_Essay_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llm-detect-ai-generated-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F61542%2F7516023%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T123959Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D770c04806b8ff7aa08dcd4e6f018dfd8cb12a171019a3810b2a72425e5d2777ccb88c3e2f25536cd0c28b8d4ed2358251c802d20c35779d611363bb7054d50785af9c290be51924b4568753242d080b6e1eab305e46d13eb49820d40bd47d9aa75998478f7b8bcb440ce7aab32347b440ce13cf406722615b78a359ca3d974cb14f8a4a2c1d4da3aefb21bd53fa8cf0fbfc92fba23617845dd1d81d7a08812dd115eb8b32333d03593a7a06e0f5847ce32d1b624693e6075dd438e61963e54765afa89d0690397575bc90edd394fd1312db3343e4cc19c6cc9aee92084eb33f6fc6dba46d0334bf726836a1955efef873373d767496dcdf233415f507e89443d,daigt-misc:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3945154%2F6865136%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T123959Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D261921bc4e57e592ea45fec19f25772f36163d540039423a72abd29fe8a433fdebedc5d58775c196f11f380aa9bdc71f6235ae1f656500a1bfa2cd27c09cac6c3cf91ee10e0d6721b22bf78db4ea909626306d2b868c0d7e1f0344b10f82a6844488c18bf15cb04cafa02cdc06bdae1751529e5cb6534a586faeb60ffbc9f25da64a2613c7bd42b31fb6f1b30cda63e3e039a5d8f6d858071cd797044b0d06807a701ca7a8be0440fe2cabd9c51b1783d9208ba3d746a91668cb440f1dc47d0b99dc459dab1360ebef199029412c86fea6d050e974736229e6d21f5a806ecf81e1855dacbda59c8adc9757b9fc2b9a4d6d85700d0b2ddf258a4b30ec471ee6e8,daigt-proper-train-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3942644%2F6890527%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T123959Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8ac226be90a13e04ab2150bfa07304ec11c83499d28bb9de452b6b01ed0036999f11bb1f1635638d84673caa4041c333a7bb88be1c7d0b76214e6e741f95c7dd1dd85731660321cfbd30a4a658013663b47bc5d66d87735c9359d50926ea95f1cffd42f1cd791bd49e3ef09fa0e592c42a28510768aa7af647aaadf26277f80f69de06e8cf2ed32220e9bc90db80ab38b0c373218aeb79b9a361234649888d1dfb0e261b2397d8e06dd777b1cda59490637a48a5d3da1032b22399823c99ef1308f43d4f96a972ba7892dbadd2cf4cabb53b9abb9c870eab34da51da222dc0f6d352950012332fc574d285adc2b7ca87711820a58e1ba401eb14ba0f4ca96cde,daigt-external-train-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3960967%2F6901341%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T123959Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Dbca73bf71e1b27468e863a1762f1fe0424763858f58e0181169ebdfbb833fec2d8f8c9d3bca50f3f4c14d1730739a86a09e1fd9e3f67e994f5d97a89a073925b5a1d683582a25bb67b082bb6d42d0a42ef3c3d209574f44fa422a71932ad507ddd8f131d233cf5014f1116880ee38ad29a85a0e6bad9ee8a8c3b5173845a4b0fb821c58e2aff93535baa9cf0f9a9cbc251e4469b9001633d88ef75f5948cf93e5134259a25fa7e1b7880ed6ed0a84075274b015550e4182510caa323c364423723c464577f62cda4b9ef6ff0756e0250c6bb9ce3dd7dba989aa41f82ad02edae9b7061f8d68b157b231430e5cbc694e84be98c711e99bc4bf3c72a2e26123d7d,daigt-v2-train-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4005256%2F6977472%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T123959Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7038270ffdcc44167bd3c8a4f2c500725b23a6e5fddf1634db97e00d2977ed01f757f7f8f770a7de8439c1840639fdcd1f7a5f257a4b28d0cd1dc8e48e73e26fd03174c91a7594c47ff01c36f10840daf229e6f6eaa6e3493a7cf48d2491685135a4e953f6f9e0002b250628909e710e254013586ca9419c6d97b6929d9993e763c507c40b01739387809688dd3f878f8ec44d14048a0e730f27c4f39c5e245f930bba13d40ac0151f7368d12ff0626c08e32679f0dce49bcfb21506e9a5a2d105efdd017a8fccd0458d43e24e0438bf370266b1709ab5bbfafbec09a43fbdfca4fdd1e598360690e11f6d14249156414a538cae14cf75519c5b660d3e9662fd'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "OiTzkB97LAkj"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "JYlX0VOaLAkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from datasets import Dataset\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# Replace the PyArrow import with pickle\n",
        "import pickle\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T15:01:32.328176Z",
          "iopub.execute_input": "2024-01-08T15:01:32.328577Z",
          "iopub.status.idle": "2024-01-08T15:01:32.335664Z",
          "shell.execute_reply.started": "2024-01-08T15:01:32.328544Z",
          "shell.execute_reply": "2024-01-08T15:01:32.334699Z"
        },
        "trusted": true,
        "id": "Ci2mOR5JLAk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "3SUt4F-5LAk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file paths\n",
        "test_path = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'\n",
        "sub_path = '/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv'\n",
        "org_train_path = '/kaggle/input/llm-detect-ai-generated-text/train_essays.csv'\n",
        "train_path = '/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv'\n",
        "\n",
        "# Read CSV files\n",
        "test = pd.read_csv(test_path)\n",
        "sub = pd.read_csv(sub_path)\n",
        "org_train = pd.read_csv(org_train_path)\n",
        "train = pd.read_csv(train_path, sep=',')\n",
        "\n",
        "# Optionally, you can print some information about the loaded data\n",
        "print(\"Test Data Shape:\", test.shape)\n",
        "print(\"Submission Data Shape:\", sub.shape)\n",
        "print(\"Original Train Data Shape:\", org_train.shape)\n",
        "print(\"Train Data Shape:\", train.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T14:54:32.494972Z",
          "iopub.execute_input": "2024-01-08T14:54:32.495606Z",
          "iopub.status.idle": "2024-01-08T14:54:34.93012Z",
          "shell.execute_reply.started": "2024-01-08T14:54:32.495577Z",
          "shell.execute_reply": "2024-01-08T14:54:34.929167Z"
        },
        "trusted": true,
        "id": "0KvCydXcLAk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows based on the 'text' column\n",
        "train = train.drop_duplicates(subset=['text']).reset_index(drop=True)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T14:54:34.93242Z",
          "iopub.execute_input": "2024-01-08T14:54:34.932721Z",
          "iopub.status.idle": "2024-01-08T14:54:35.008231Z",
          "shell.execute_reply.started": "2024-01-08T14:54:34.932696Z",
          "shell.execute_reply": "2024-01-08T14:54:35.007347Z"
        },
        "trusted": true,
        "id": "A34zIU5MLAk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Parameter"
      ],
      "metadata": {
        "id": "kXJVJg5zLAk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "LOWERCASE = False\n",
        "VOCAB_SIZE = 30522\n",
        "\n",
        "# Your code continues...\n",
        "\n",
        "# For example, you can use these parameters like this:\n",
        "if LOWERCASE:\n",
        "    # Do something when LOWERCASE is True\n",
        "    pass\n",
        "else:\n",
        "    # Do something when LOWERCASE is False\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T14:54:35.009305Z",
          "iopub.execute_input": "2024-01-08T14:54:35.009598Z",
          "iopub.status.idle": "2024-01-08T14:54:35.015852Z",
          "shell.execute_reply.started": "2024-01-08T14:54:35.009562Z",
          "shell.execute_reply": "2024-01-08T14:54:35.015103Z"
        },
        "trusted": true,
        "id": "0uW0qRaVLAlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Byte-Pair Encoding (BPE) tokenizer"
      ],
      "metadata": {
        "id": "u5Gw5pY_LAlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dataset from the 'test' DataFrame\n",
        "dataset = Dataset.from_pandas(test[['text']])\n",
        "\n",
        "# Define train_corp_iter function\n",
        "def train_corp_iter():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]\n",
        "\n",
        "# Creating Byte-Pair Encoding tokenizer\n",
        "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
        "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
        "\n",
        "# Use the train_corp_iter function to train the tokenizer\n",
        "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
        "\n",
        "# Create a PreTrainedTokenizerFast object\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=raw_tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")\n",
        "\n",
        "# Tokenize texts for test and train datasets\n",
        "tokenized_texts_test = [tokenizer.tokenize(text) for text in tqdm(test['text'].tolist())]\n",
        "tokenized_texts_train = [tokenizer.tokenize(text) for text in tqdm(train['text'].tolist())]\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T14:54:35.017102Z",
          "iopub.execute_input": "2024-01-08T14:54:35.017428Z",
          "iopub.status.idle": "2024-01-08T14:56:46.054742Z",
          "shell.execute_reply.started": "2024-01-08T14:54:35.017398Z",
          "shell.execute_reply": "2024-01-08T14:56:46.053741Z"
        },
        "trusted": true,
        "id": "7rQizqR3LAlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vectorization"
      ],
      "metadata": {
        "id": "E5gty24YLAlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def dummy(text):\n",
        "    return text\n",
        "\n",
        "# Create TF-IDF vectorizer with vocabulary from test data\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(3, 5),\n",
        "    lowercase=False,\n",
        "    sublinear_tf=True,\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy,\n",
        "    preprocessor=dummy,\n",
        "    token_pattern=None,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "\n",
        "# Fit on test data to get vocabulary\n",
        "vectorizer.fit(tokenized_texts_test)\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "# Print vocabulary (optional)\n",
        "print(vocab)\n",
        "\n",
        "# Use the obtained vocabulary for vectorizing train and test datasets\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(3, 5),\n",
        "    lowercase=False,\n",
        "    sublinear_tf=True,\n",
        "    vocabulary=vocab,\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy,\n",
        "    preprocessor=dummy,\n",
        "    token_pattern=None,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "\n",
        "# Transform train and test datasets\n",
        "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
        "tf_test = vectorizer.transform(tokenized_texts_test)\n",
        "\n",
        "# Cleanup\n",
        "del vectorizer\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T14:56:46.055999Z",
          "iopub.execute_input": "2024-01-08T14:56:46.056285Z",
          "iopub.status.idle": "2024-01-08T15:00:56.772818Z",
          "shell.execute_reply.started": "2024-01-08T14:56:46.05626Z",
          "shell.execute_reply": "2024-01-08T15:00:56.771879Z"
        },
        "trusted": true,
        "id": "Vr7bq1jQLAlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label'].values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T15:00:56.774483Z",
          "iopub.execute_input": "2024-01-08T15:00:56.774894Z",
          "iopub.status.idle": "2024-01-08T15:00:56.77936Z",
          "shell.execute_reply.started": "2024-01-08T15:00:56.774861Z",
          "shell.execute_reply": "2024-01-08T15:00:56.778515Z"
        },
        "trusted": true,
        "id": "qCS9wsmDLAlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Submission"
      ],
      "metadata": {
        "id": "1qoiIHcnLAlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import gc\n",
        "\n",
        "# Check if the number of samples in the test set is less than or equal to 5\n",
        "if len(test.text.values) <= 5:\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "else:\n",
        "    # Define individual classifiers\n",
        "    clf = MultinomialNB(alpha=0.02)\n",
        "    sgd_model = SGDClassifier(max_iter=8500, tol=1e-4, loss=\"modified_huber\")\n",
        "\n",
        "    lgb_params = {\n",
        "        'n_iter': 2480,\n",
        "        'verbose': -1,\n",
        "        'objective': 'cross_entropy',\n",
        "        'metric': 'auc',\n",
        "        'learning_rate': 0.0058,\n",
        "        'colsample_bytree': 0.78,\n",
        "        'colsample_bynode': 0.8,\n",
        "        'lambda_l1': 4.56,\n",
        "        'lambda_l2': 2.97,\n",
        "        'min_data_in_leaf': 114,\n",
        "        'max_depth': 24,\n",
        "        'max_bin': 898\n",
        "    }\n",
        "    lgb = LGBMClassifier(**lgb_params)\n",
        "\n",
        "    cat_params = {\n",
        "        'iterations': 2010,\n",
        "        'verbose': 0,\n",
        "        'l2_leaf_reg': 6.659,\n",
        "        'learning_rate': 0.00560,\n",
        "        'subsample': 0.4,\n",
        "        'allow_const_label': True,\n",
        "        'loss_function': 'CrossEntropy'\n",
        "    }\n",
        "    cat = CatBoostClassifier(**cat_params)\n",
        "\n",
        "    # Define weights for the voting classifier\n",
        "    weights = [0.072, 0.31, 0.309, 0.309]\n",
        "\n",
        "    # Create a voting classifier\n",
        "    ensemble = VotingClassifier(estimators=[\n",
        "        ('mnb', clf),\n",
        "        ('sgd', sgd_model),\n",
        "        ('lgb', lgb),\n",
        "        ('cat', cat)\n",
        "    ], weights=weights, voting='soft', n_jobs=-1)\n",
        "\n",
        "    # Train the ensemble on the training set\n",
        "    ensemble.fit(tf_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    final_preds = ensemble.predict_proba(tf_test)[:, 1]\n",
        "\n",
        "    # Add the predictions to the submission DataFrame\n",
        "    sub['generated'] = final_preds\n",
        "\n",
        "    # Save the submission to a CSV file\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "\n",
        "    # Cleanup\n",
        "    del ensemble, clf, sgd_model, lgb, cat\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T15:00:56.780674Z",
          "iopub.execute_input": "2024-01-08T15:00:56.780951Z",
          "iopub.status.idle": "2024-01-08T15:00:56.794865Z",
          "shell.execute_reply.started": "2024-01-08T15:00:56.780916Z",
          "shell.execute_reply": "2024-01-08T15:00:56.794042Z"
        },
        "trusted": true,
        "id": "WN4bVlD5LAlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the submission results\n",
        "print(sub.head())\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-08T15:00:56.797469Z",
          "iopub.execute_input": "2024-01-08T15:00:56.797779Z",
          "iopub.status.idle": "2024-01-08T15:00:56.811389Z",
          "shell.execute_reply.started": "2024-01-08T15:00:56.797754Z",
          "shell.execute_reply": "2024-01-08T15:00:56.810558Z"
        },
        "trusted": true,
        "id": "VCVCsq2VLAlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}